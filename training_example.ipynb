{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9492829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_line_follower\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "\n",
    "from stable_baselines3 import DDPG, PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "from stable_baselines3.common.noise import OrnsteinUhlenbeckActionNoise\n",
    "import numpy as np\n",
    "\n",
    "from wrappers import RenderRerun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7107a493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_environment(filename: str | None = None, skip_episodes=1000, viewer=\"notebook\"):\n",
    "    env = gym.make('LineFollower-v0', gui = False, render_mode = 'rgb_array')\n",
    "    env = RenderRerun(env, filename=filename, skip_episodes=skip_episodes, viewer=viewer)\n",
    "\n",
    "    vec_env = DummyVecEnv([lambda: env])\n",
    "    return env, vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b91564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = True\n",
    "model_type = \"ppo\" # \"ppo\" or \"ddpg\n",
    "\n",
    "model_name = model_type +\"_line_follower\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SKDfs0RAghan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize_environment should run separately, otherwise the viewer does not refresh!\n",
    "env, vec_env = initialize_environment(filename=\"gym-line-follower_training.rrd\", skip_episodes=25, viewer=\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccce9df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model:\n",
    "    if model_type == \"ddpg\":\n",
    "        # Stop training if there is no improvement after more than 3 evaluations\n",
    "        stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=10, min_evals=5, verbose=1)\n",
    "        eval_callback = EvalCallback(env, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=1)\n",
    "        # The noise objects for DDPG\n",
    "        n_actions = env.action_space.shape[0]\n",
    "        action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "        model = DDPG(\"MlpPolicy\", vec_env, action_noise=action_noise, verbose=1, tensorboard_log=\"./ddpg_line_follower_tensorboard/\")\n",
    "\n",
    "        # model = DDPG(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./ppo_line_follower_tensorboard/\")\n",
    "        model.learn(total_timesteps=10000, callback=eval_callback)\n",
    "\n",
    "        # Evaluate the agent\n",
    "        mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=100)\n",
    "        print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "        # # Save the agent\n",
    "        # model.save(\"ddpg_line_follower\")\n",
    "\n",
    "    if model_type == \"ppo\":\n",
    "        # Stop training if there is no improvement after more than 3 evaluations\n",
    "        stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=10, min_evals=5, verbose=1)\n",
    "        eval_callback = EvalCallback(env, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=1)\n",
    "\n",
    "        model = PPO(\"MlpPolicy\", vec_env, verbose=1, tensorboard_log=\"./ppo_line_follower_tensorboard/\")\n",
    "        model.learn(total_timesteps=100_000, callback=eval_callback)\n",
    "\n",
    "        # Evaluate the agent\n",
    "        mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=100)\n",
    "        print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "    #save the model\n",
    "    model.save(model_name)\n",
    "\n",
    "else:\n",
    "    if model_type == \"ddpg\":\n",
    "       # The noise objects for DDPG\n",
    "        n_actions = env.action_space.shape[0]\n",
    "        action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "        model = DDPG(\"MlpPolicy\", vec_env, action_noise=action_noise, verbose=1, tensorboard_log=\"./ddpg_line_follower_tensorboard/\")\n",
    "\n",
    "    if model_type == \"ppo\":\n",
    "        model = PPO(\"MlpPolicy\", vec_env, verbose=1, tensorboard_log=\"./ppo_line_follower_tensorboard/\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9588688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new environment for testing, saving a new recording\n",
    "env, vec_env = initialize_environment(filename=\"gym-line-follower_training.rrd\", skip_episodes=0, viewer=\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5ba469",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the trained agent and do a test run\n",
    "if model_type == \"ppo\":\n",
    "    model = PPO.load(\"ppo_line_follower\", env=vec_env)\n",
    "if model_type == \"ddpg\":\n",
    "    model = DDPG.load(\"ddpg_line_follower\", env=vec_env)\n",
    "\n",
    "obs = model.env.reset()\n",
    "\n",
    "steps = 0\n",
    "for i in range(1000):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    # print(\"Action: \", *action)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "\n",
    "    steps += 1\n",
    "    if done:\n",
    "        print(\"Done in \", steps, \" steps\")\n",
    "        break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rerun-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
