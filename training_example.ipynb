{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9492829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_line_follower\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "\n",
    "from stable_baselines3 import DDPG, PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "from stable_baselines3.common.noise import OrnsteinUhlenbeckActionNoise\n",
    "import numpy as np\n",
    "\n",
    "from wrappers import RenderRerun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7107a493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_environment(filename: str = None, skip_episodes=1000):\n",
    "    env = gym.make('LineFollower-v0', gui = False, render_mode = 'rgb_array')\n",
    "    env = RenderRerun(env, filename=filename, skip_episodes=skip_episodes, viewer=\"notebook\")\n",
    "\n",
    "    vec_env = DummyVecEnv([lambda: env])\n",
    "    return env, vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b91564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = True\n",
    "model_type = \"ppo\" # \"ppo\" or \"ddpg\n",
    "\n",
    "model_name = model_type +\"_line_follower\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccce9df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env, vec_env = initialize_environment(filename=\"gym-line-follower_training.rrd\", skip_episodes=500)\n",
    "\n",
    "if train_model:\n",
    "    if model_type == \"ddpg\":\n",
    "        # Stop training if there is no improvement after more than 3 evaluations\n",
    "        stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=10, min_evals=5, verbose=1)\n",
    "        eval_callback = EvalCallback(env, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=1)\n",
    "        # The noise objects for DDPG\n",
    "        n_actions = env.action_space.shape[0]\n",
    "        action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "        model = DDPG(\"MlpPolicy\", vec_env, action_noise=action_noise, verbose=1, tensorboard_log=\"./ddpg_line_follower_tensorboard/\")\n",
    "\n",
    "        # model = DDPG(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./ppo_line_follower_tensorboard/\")\n",
    "        model.learn(total_timesteps=10000, callback=eval_callback)\n",
    "\n",
    "        # Evaluate the agent\n",
    "        mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=100)\n",
    "        print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "        # # Save the agent\n",
    "        # model.save(\"ddpg_line_follower\")\n",
    "\n",
    "    if model_type == \"ppo\":\n",
    "        # Stop training if there is no improvement after more than 3 evaluations\n",
    "        stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=10, min_evals=5, verbose=1)\n",
    "        eval_callback = EvalCallback(env, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=1)\n",
    "\n",
    "        model = PPO(\"MlpPolicy\", vec_env, verbose=1, tensorboard_log=\"./ppo_line_follower_tensorboard/\")\n",
    "        model.learn(total_timesteps=100_000, callback=eval_callback)\n",
    "\n",
    "        # Evaluate the agent\n",
    "        mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=100)\n",
    "        print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "    #save the model\n",
    "    model.save(model_name)\n",
    "\n",
    "else:\n",
    "    if model_type == \"ddpg\":\n",
    "       # The noise objects for DDPG\n",
    "        n_actions = env.action_space.shape[0]\n",
    "        action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "        model = DDPG(\"MlpPolicy\", vec_env, action_noise=action_noise, verbose=1, tensorboard_log=\"./ddpg_line_follower_tensorboard/\")\n",
    "        \n",
    "    if model_type == \"ppo\":\n",
    "        model = PPO(\"MlpPolicy\", vec_env, verbose=1, tensorboard_log=\"./ppo_line_follower_tensorboard/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e5ba469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "options= \n",
      "made client\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\Projects\\rerun_demo\\.venv\\Lib\\site-packages\\gymnasium\\spaces\\box.py:236: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "c:\\Users\\andre\\Projects\\rerun_demo\\.venv\\Lib\\site-packages\\gymnasium\\spaces\\box.py:306: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b9781681f64eabac3cd38b5b40f146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<div id=\"63c0b237-1682-4cc5-b5ad-546a9e0be17b\"><style onload=\"eval(atob(\\'KGFzeW5jIGZ1bmN0aW9uICgpâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b97a1489a0149dfac2fee61eaf94abb",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Viewer()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a new environment for testing, saving a new recording file\n",
    "env, vec_env = initialize_environment(filename=\"gym-line-follower_test.rrd\", skip_episodes=0)\n",
    "\n",
    "# Load the trained agent and do a test run\n",
    "if model_type == \"ppo\":\n",
    "    model = PPO.load(\"ppo_line_follower\", env=vec_env)\n",
    "if model_type == \"ddpg\":\n",
    "    model = DDPG.load(\"ddpg_line_follower\", env=vec_env)\n",
    "\n",
    "obs = model.env.reset()\n",
    "\n",
    "steps = 0\n",
    "for i in range(10):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    # print(\"Action: \", *action)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "\n",
    "    steps += 1\n",
    "    if done:\n",
    "        print(\"Done in \", steps, \" steps\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0403970",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rerun-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
